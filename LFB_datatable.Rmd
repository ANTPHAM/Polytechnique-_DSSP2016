---
title: "London Fire Brigad with data.table"
author: "Erwan Le Pennec"
date: "8 March 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache = TRUE, autodep = TRUE, tidy = FALSE)
```
In this lab, we will work with a dataset provided by the London Fire Brigad which contains around 800 000 incident records from January 2009 to January 2015. 
The original dataset is available at http://data.london.gov.uk/dataset/london-fire-brigade-incident-records as two xlsx files, which are also in the moodle. 

Note that, as often in Data Science, there are several ways to do the same thing. This script is a variation around the lab one using __data.table__, a faster alternative to the data.frames.


# Reading the data

## A first read

The first step is to read the files into __R__. We will use __fread__ which has a quite straightforward interface but is only adapted to __csv__ files. We can use __xlsx2csv__ from __anaconda__ to convert the __xlsx__s into __csv__s. 

```{r, engine = "bash", eval = FALSE}
xlsx2csv "SQL LFB Incident data from Jan2009 - Dec2012.xlsx"
xlsx2csv "201601 SQL LFB Incident data from Jan2013 - Jan2016.xlsx"
```


```{r Read_First}
pacman::p_load("data.table")
LFB1 <- fread("SQL LFB Incident data from Jan2009 - Dec2012.csv")
str(LFB1)
```

The resulting object is a data.table, which is an array like structure used in R to store columnar data. Internally, it is stored as a list of columns of identical size. This structure is linked to the __data.table__ package and is very efficient for very large datasets. The key is that is used a clever indexing scheme and depart from the functional logic of __R__ by allowing in place modifications.


__RStudio__ includes a basic viewer to __View__ a data.frame.

```{r View, eval = FALSE}
View(LFB1)
```

Several observations can be made:

- The first column is an id that should be an integer.
- The __TimeOfCall__ seems strange.
- There a few __"NULL"__ values that seems to correspond to a __NA__.

We will see now how to fix those issues.

## A second read


We first cope with the __"NULL"__ strings and the id columns using  dedicated options from __fread__.

```{r Read}
LFB1 <- fread("SQL LFB Incident data from Jan2009 - Dec2012.csv", 
              na.strings = "NULL",
              colClasses = list(integer=1))
LFB2 <- fread("201601 SQL LFB Incident data from Jan2013 - Jan2016.csv",
              na.strings = "NULL",
              colClasses = list(integer=1))

LFB <- rbindlist( list(LFB1,LFB2))
rm(LFB1,LFB2)
```

```{r Check}
str(LFB)
```


## Modifying the columns

We fix the time using the _pipeline_ system of __data.table__:

```{r FixTime}
pacman::p_load("lubridate")
LFB[, DateOfCall := dmy(DateOfCall, tz = "UTC") + hm(TimeOfCall)][,TimeOfCall := NULL]
```


An important concept in __R__ is the concept of factor which corresponds to a variable that can take a finite number of values. Looking at the data, one can see that this is the case for all the variables stored as text.  


```{r FixFactor}
upd.cols = sapply(LFB, is.character)
LFB[, names(LFB)[upd.cols] := lapply(.SD, factor), .SDcols = upd.cols]
```

```{r CheckFactor}
str(LFB)
```

## A last detail


A very close inspection of the __levels__ of __IncGeo\_BoroughName__ shows a last issue:


```{r Levels}
levels(LFB$IncGeo_BoroughName)
```

The names are slightly different in the two xlsx files! We can fix this issue by normalizing all the names.

```{r FixFactor2}
LFB[, `:=`(IncGeo_BoroughName = stringr::str_to_title(IncGeo_BoroughName),
           IncGeo_WardName = stringr::str_to_title(IncGeo_WardName))]

upd.cols = sapply(LFB, is.character)
LFB[, names(LFB)[upd.cols] := lapply(.SD, factor), .SDcols = upd.cols]
```

```{r LFB}
LFB
```

# Dataframe manipulation

__data.table__ allows to manipulate the dataset in way quite similar to a SQL database through a unified __DT[i, j, by]__ syntax.

## A global view

We have already seen the __View__ command to look at a dataset. Other options include
```{r Head}
head(LFB)
```
to look at the first entries or, to obtain a better display in a __Rmd__,
```{r Kabble, results= 'asis'}
knitr::kable(head(LFB))
```

Note that if you don't use the __Rmd__ file then the first display is better...

If one wants to look at subpart of the dataset, one can filter the data.table using the __i__ in __DT[i, j, by]__.

```{r Filter1, results= 'asis'}
LFBMonday <- LFB[wday(DateOfCall, label = TRUE) == "Mon",]
knitr::kable(head(LFBMonday))
rm(LFBMonday)
```

__Exercise:__ How to obtain the subset corresponding to False Alarms on a week-end day?

```{r FilterExo, results = 'asis'}
LFBExo <- LFB[(IncidentGroup == "False Alarm")& (wday(DateOfCall, label = TRUE) %in% c("Sat","Sun")),]
knitr::kable(head(LFBExo))
rm(LFBExo)
```

## Summarizing the data

_R__ includes a very convenient command to __summarize__ a dataset:
```{r Summary}
summary(LFB)
```

It computes a lot of statistics for each column of the dataset. Note that the statistics computed depends on the type of the column.

Those statistics can be computed _manually_ using the __j__ in __DT[i, j, by]__:

```{r Summarize}
LFB[,.(medNumPumpsAttending = median(NumPumpsAttending, na.rm = TRUE), meanNumPumpsAttending = mean(NumPumpsAttending, na.rm = TRUE))]
LFB[,.(NbIncidentGroup = .N, NAIncidentGroup = sum(is.na(IncidentGroup)))]
```

This command can be combined with a filter to compute a statistics on a subset:
```{r FilterSummarize}
LFBIncidentGroup <- LFB[wday(DateOfCall, label = TRUE) == "Mon",
                        .(NbIncidentGroup = .N, NAIncidentGroup = sum(is.na(IncidentGroup)))]
```


__Exercise:__ Compute the average __FirstPumpArriving_AttendanceTime__ on __Fire__ events during the week-end and during the week.

```{r FilterSummarizeExo}
LFB[(IncidentGroup == "Fire") &
  (wday(DateOfCall, label = TRUE) %in% c("Sat","Sun")),
  .(meanTime = mean(FirstPumpArriving_AttendanceTime, na.rm = TRUE))]
LFB[(IncidentGroup == "Fire") & !(wday(DateOfCall, label = TRUE) %in% c("Sat","Sun")),
    .(meanTime = mean(FirstPumpArriving_AttendanceTime, na.rm = TRUE))]
```

## Split-Apply-Combine strategy

The Split-Apply-Combine strategy is a very natural way to process a dataset:
1. Split the data in groups
2. Apply a procedure on each of those groups
3. Combine those results

We will see that __R__ and __data.table__ are well suited to do this thanks to the __by__ of __DT[i, j, by]__:


```{r GroupBySummarize}
LFBIncidentGroup <- LFB[,.(N = .N), by = IncidentGroup]
LFBIncidentGroup
```


__Exercise:__ Compute the average __FirstPumpArriving_AttendanceTime__ on __Fire__ events during the week-end and during the week in a single command.

```{r GroupBySummarizeExo}
LFB[IncidentGroup == "Fire",
    .(meanTime = mean(FirstPumpArriving_AttendanceTime, na.rm = TRUE)),
    by = .( weekend = (wday(DateOfCall, label = TRUE) %in% c("Sat","Sun")))]
```

Note that the operations can be chained. For instance, to compute the proportion of each IncidentGroup by weekday:

```{r SummarizeChain}
LFBProp <- LFB[,.(N = .N), by = .(wday = wday(DateOfCall, label = TRUE), IncidentGroup)][, prop :=  N/sum(N), by = .(wday)]
LFBProp
```

__Exercise:__ Compute the mean and the standard deviation of the number of incidents by day for each IncidentGroup.

```{r SummarizeChainExo}
LFB[, .(N = .N), by= . (IncidentGroup, day = day(DateOfCall))][, .(meanN = mean(N), sdN = sd(N)), by = IncidentGroup]
```

## Reshaping the data

We conclude our tour of data manipulation by showing how to go from a _long_ table to a _wide_ table... and the other way around.

A _long_ table is a table in which each line corresponds to a _measurement_. This is the most convenient way to deal with the data but is far from being efficient from a storage point of view. An example is given by the following table in which the number of events by IncidentGroup and Weekday is stored:


```{r LongTable}
LFBIncidentGroupWday <- LFB[,.(N= .N),
                            by =.(IncidentGroup,
                                  wday = wday(DateOfCall, label = TRUE))]
LFBIncidentGroupWday
```

The __data.table__ package provides a command __dcast__ to transform this table in a _wide_ format in which in each line appears several related measurements.


```{r Dcast}
LFBIncidentGroupWdayW <- dcast(LFBIncidentGroupWday, IncidentGroup ~ wday, fill = 0)
LFBIncidentGroupWdayW
```


We can go back to the original table thanks to the __melt__ command:

```{r Melt}
LFBIncidentGroupWday2 <- melt(LFBIncidentGroupWdayW, "IncidentGroup", variable.name = "wday", value.name = "N")
LFBIncidentGroupWday2
```

__Exercise:__ Compute and display in a table the number of incidents by weekday and hour by IncidentGroup and globally.


```{r GroupWdayHour, results = 'asis'}
LFBWdayHourIncidentGroup <- LFB[, .(N = .N),
    by= .(wday = wday(DateOfCall, label = TRUE), hour = hour(DateOfCall), IncidentGroup)]


LFBWdayHour <- LFBWdayHourIncidentGroup[,.(N = sum(N)),
                                        by = .(wday, hour)]


LFBWdayHourIncidentGroupW <- lapply(levels(LFB$IncidentGroup),
      function(x) {
          list(x, dcast(LFBWdayHourIncidentGroup[IncidentGroup == x], wday ~ hour, fill = 0))         })

LFBWdayHourW <- dcast(LFBWdayHour, wday ~ hour, fill = 0)

for (i in seq_along(LFBWdayHourIncidentGroupW)) {
print(knitr::kable(LFBWdayHourIncidentGroupW[[i]][2],
             caption = LFBWdayHourIncidentGroupW[[i]][1]))
}

knitr::kable(LFBWdayHourW)
```

## Join


Joining two tables together is also possible in __R__ for example with the __DT[DT2, i , j]__ syntax of data.table.

We will see some example during the next lab.

# Visualization


In this part, we will show a few visualization examples ranging from basic to quite advanced one. We will use the very powerful __ggplot2__ package.

## Table and bar plots

Bar plots are a convenient way to visualize tables.

```{r Histo}
pacman::p_load("ggplot2")
ggplot(data = LFBIncidentGroup, aes(x = IncidentGroup, y = N, fill = IncidentGroup)) + geom_bar(stat = "identity")
```


Note that the very same plot could have been computed directly from the original dataset:

```{r HistoFromScratch}
ggplot(data = LFB, aes(x = IncidentGroup, fill = IncidentGroup)) + geom_bar()
```

The summary that we had computed manually is computed automatically in this case...

We can look at the variation with respect to the day by 

- faceting the data

```{r HistoFacet}
ggplot(data = LFB[, wday := wday(DateOfCall, label = TRUE)],
       aes(x = IncidentGroup, fill = IncidentGroup)) +
  geom_bar() + facet_wrap(~ wday)
```

- stacking the bars

```{r HistoWday}
ggplot(data = LFB,
       aes(x = wday, fill = IncidentGroup)) +
  geom_bar()
```

- or looking at the conditional proportions...

```{r HistoWdayFill}
ggplot(data = LFB,
       aes(x = wday, fill = IncidentGroup)) +
  geom_bar(position = "fill")
```


__Exercise:__ What about the repartition of the __IncGeo\_BoroughName__?

```{r Borough}
ggplot(data = LFB, aes(x = IncGeo_BoroughName)) + geom_bar() 

ggplot(data = LFB[, .(N = .N), by= .(IncGeo_BoroughName)], aes(x = reorder(IncGeo_BoroughName,N), y = N)) + geom_bar(stat = "identity") + coord_flip()

ggplot(data = LFB[, .(N = .N), by= .(IncGeo_BoroughName)], aes(x = reorder(IncGeo_BoroughName,N), y = N)) + geom_point() + coord_flip()
```

## Maps


__OpenStreetMap__ is a package that can be combined with__ggplot2__ to display maps. We will use them after a tedious conversion of the coordinates into the metric system...

```{r Conversion, cache.vars = "LFB"}
wgs84 = "+init=epsg:4326"
bng = '+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs'

ConvertCoordinates <- function(easting,northing) {
  out = cbind(long_tmp = easting,lat_tmp = northing)
  mask = !is.na(easting)
  sp <- sp::spTransform(sp::SpatialPoints(list(easting[mask],northing[mask]),proj4string=sp::CRS(bng)),sp::CRS(wgs84))
  out[mask,]=sp@coords
  list(out[,1], out[,2])
}

LFB[ ,c("long","lat") := ConvertCoordinates(Easting_m, Northing_m)]
LFB[ ,c("long_rounded","lat_rounded") := ConvertCoordinates(Easting_rounded, Northing_rounded)]
```

We can now use __OpenStreetMap__ to get a London map
```{r OpenStreetMap}
pacman::p_load("OpenStreetMap")
pacman::p_load("ggplot2")
mapLondon <- openmap(c(51.75,-0.6),c(51.25,.4))
mapLondon <- openproj(mapLondon)
autoplot(mapLondon) + coord_quickmap(expand = FALSE, xlim = c(-0.6,.4),
                                                                     ylim = c(51.25,51.75)) 
```

We can add the events on the map: 
```{r MapEvents}
autoplot(mapLondon) + geom_point(data = LFB, aes(x = long_rounded, y = lat_rounded, color = IncidentGroup)) +
  coord_quickmap(expand = FALSE, xlim = c(-0.6,.4),
                                                                     ylim = c(51.25,51.75))
```
and enhance the result thanks to the use of transparency

```{r MapEventsAlpha}
autoplot(mapLondon) + geom_point(data = LFB, aes(x = long_rounded, y = lat_rounded,
                                              color = IncidentGroup),
                              alpha = .01) +
  scale_color_discrete(guide = guide_legend(override.aes = list(alpha = 1))) +
  coord_quickmap(expand = FALSE, xlim = c(-0.6,.4),
                                                                     ylim = c(51.25,51.75))

autoplot(mapLondon) + geom_point(data = LFB,
                              aes(x = long_rounded, y = lat_rounded,
                                  color = IncidentGroup),
                              alpha = .01) +
  scale_color_discrete(guide = guide_legend(override.aes = list(alpha = 1))) + 
  coord_quickmap(expand = FALSE, xlim = c(-0.6,.4),
                                                                     ylim = c(51.25,51.75)) +
  facet_wrap(~ IncidentGroup)
```

__Exercise:__ Can you obtain a better view?
```{r OpenStreetMap2}
mapLondon <- openmap(c(51.57,-0.21),c(51.43,.01))
mapLondon <- openproj(mapLondon)
autoplot(mapLondon) + coord_quickmap(expand = FALSE, xlim = c(-0.21,.01),
                                                                     ylim = c(51.43,51.57))
```

## A much more complex visualization

We may be interested by a visualization of the number of hourly incidents for a given date with respect to the _usual_ number of hourly incidents. A classical way to visualize this is to compute the quantiles of the hourly, to display them on a graph and to overlay the number for a given day.

__ggplot2__ allows a construction of such a graph as soon as the quantiles are computed.

We need first to compute the number of events for every hour:

```{r EventsByHour}
LFBHour <- LFB[, .(N = .N), by= .(DateOfCall = floor_date(DateOfCall, "hour"))][, `:=`(hour = hour(DateOfCall), DayOfCall = floor_date(DateOfCall, "day"))]

setkey(LFBHour, hour, DayOfCall)

LFBHour <- LFBHour[CJ(hour, DayOfCall, unique = TRUE), .(DateOfCall = force_tz(DayOfCall + dhours(hour), "UTC"), N)][is.na(N), N:=0]
```

Note that the absence of value in the first step for a given hour means that there was no events and not that there is a missing value, hence the second part.

The quantiles can be computed by this (not so simple) sequence of commands.

```{r Quant}
Probs <- seq(0,1,.01)
Quant <- LFBHour[ , .(Quant = quantile(N, Probs), Probs = Probs), by = .(hour = hour(DateOfCall))][, `:=`(QuantN = shift(Quant, type = "lead"), ProbsN = shift(Probs, type = "lead")), by = hour]
```

A first visualization of the quantiles is obtained easily:

```{r Plot}
ggplot(data = Quant[, ProbsM := (Probs+ProbsN)/2]) + geom_ribbon(aes(x = hour, ymin = Quant, ymax = QuantN,
                                       fill = ProbsM,
                                       group = Probs))
```

This visualization can be enhanced thanks to the __ggplot2__ syntax:

```{r PlotEnhanced}
ProbsShort <- seq(0,1,.1)
p <- ggplot(data = Quant) + geom_ribbon(aes(x = hour, ymin = Quant, ymax = QuantN,
                                       fill = ProbsM,
                                       group = Probs, alpha = ProbsM)) +
  geom_line(data = Quant[Probs == .5], aes(x = hour, y = Quant, group = Probs), linetype = "dashed") +
  scale_fill_gradientn(values = c(0,.75,1), colors = c("green", "orange", "red"),
                       guide = guide_legend(name = "legend", title = "Quantile"), trans = 'identity', breaks = rev(ProbsShort)) +
scale_alpha_continuous(guide = guide_legend(name = "legend", title = "Quantile"),
                       trans = scales::trans_new("",function(x) { (1-x)^(1/10)}, function(x) {1-x^10}, domain = c(0,1)),
                       breaks = rev(ProbsShort)) +
  xlab("Hour") + ylab("Nb of events") + coord_cartesian(ylim = c(0,50))
p
```

We can then overlay the data for a given day:

```{r PlotOverlay}
DayOfInterest <- dmy("04/05/2015", tz = "UTC")
p + geom_line(data = LFBHour[floor_date(DateOfCall, "day") == DayOfInterest],
                             aes(x = hour(DateOfCall), y = N,
                                 group = floor_date(DateOfCall, "day"))) +
  ggtitle(sprintf("Events on %s",format(DayOfInterest,"%d/%m/%Y")))
```
